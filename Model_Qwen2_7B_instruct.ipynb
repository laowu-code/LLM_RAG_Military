{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39e503e-a1f7-4852-a6f8-f7decd09cd77",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-22T09:24:45.304152Z",
     "iopub.status.busy": "2024-07-22T09:24:45.303477Z",
     "iopub.status.idle": "2024-07-22T09:27:50.105219Z",
     "shell.execute_reply": "2024-07-22T09:27:50.104302Z",
     "shell.execute_reply.started": "2024-07-22T09:24:45.304121Z"
    },
    "tags": []
   },
   "source": [
    "# 下载模型LLM Qwen2-7B-Instruct\n",
    "import torch\n",
    "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "import os\n",
    "model_dir = snapshot_download('qwen/Qwen2-7B-Instruct', cache_dir='qwen/autodl-tmp', revision='master')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c19aa6-c58c-4529-88b1-c794cb8c89f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "配置环境\n",
    "python -m pip install --upgrade pip\n",
    "# 更换 pypi 源加速库的安装\n",
    "pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "pip install modelscope==1.9.5\n",
    "pip install \"transformers>=4.39.0\"\n",
    "pip install streamlit==1.24.0\n",
    "pip install sentencepiece==0.1.99\n",
    "pip install accelerate==0.27\n",
    "pip install transformers_stream_generator==0.0.4\n",
    "pip install datasets==2.18.0\n",
    "pip install peft==0.10.0\n",
    "\n",
    "# 可选\n",
    "MAX_JOBS=8 pip install flash-attn --no-build-isolation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c86550-a771-443b-9f9c-49f1d26fd850",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 数据处理函数，批量处理成LLM的输入，使之数据符合格式要求"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4a65866-bc24-4ece-9d84-590b942acf77",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "source": [
    "#处理输入dataset\n",
    "def process_func(example):\n",
    "    MAX_LENGTH = 384    # Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(f\"<|im_start|>system\\n现在你是一个军事策略专家，具有丰富的军事知识与经验<|im_end|>\\n<|im_start|>user\\n{example['instruction'] + example['input']}<|im_end|>\\n<|im_start|>assistant\\n\", add_special_tokens=False)  # add_special_tokens 不在开头加 special_tokens\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e456ab-7b94-47a1-81ee-f1d28f6cc7f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 训练LLM with Lora"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5be89006-c90b-4c62-8418-2f1ea662e225",
   "metadata": {},
   "source": [
    "import torch\n",
    "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import os\n",
    "\n",
    "\n",
    "df = pd.read_json('../dataset/huanhuan.json')\n",
    "ds = Dataset.from_pandas(df)\n",
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('qwen/autodl-tmp/qwen/Qwen2-7B-Instruct/', use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained('qwen/autodl-tmp/qwen/Qwen2-7B-Instruct/', device_map=\"auto\",torch_dtype=torch.bfloat16)\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1# Dropout 比例\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/Qwen2_instruct_lora\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af519a4f-30b1-4b76-8983-425f0856922b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 推理LLM with trained Lora weights"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a484bdb4-e88a-42a7-8c9b-0cd91877c14f",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "model_path = '/root/autodl-tmp/qwen/Qwen2-7B-Instruct/'\n",
    "lora_path = 'lora_path'\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\",torch_dtype=torch.bfloat16)\n",
    "\n",
    "# 加载lora权重\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path, config=config)\n",
    "\n",
    "prompt = \"你是谁？\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"现在你是一个军事策略专家，具有丰富的军事知识与经验\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf57f21-ab2f-4562-b8f7-78cbeaf157a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 部署LLM with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0aba2f-857a-45c0-bf65-afd6eda9d5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
